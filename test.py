from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import re
import json
from typing import List, Dict, Union

class Llama3ForensicAnalyzer:
    """Llama3-8Bæ‰¹é‡åˆ†æå™¨ï¼šæ¨¡å‹ä»…åŠ è½½ä¸€æ¬¡ï¼Œæ‰¹é‡å¤„ç†ä¸åŒcontextä»»åŠ¡"""
    
    def __init__(self, model_path: str, load_in_8bit: bool = True):
        """
        åˆå§‹åŒ–ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼ŒåŠ è½½æ¨¡å‹å’Œtokenizerï¼‰
        :param model_path: æœ¬åœ°Llama3-8Bæ¨¡å‹è·¯å¾„
        :param load_in_8bit: æ˜¯å¦8bité‡åŒ–ï¼ˆé»˜è®¤Trueï¼Œæ˜¾å­˜ä¸è¶³å¯æ”¹False/4bitï¼‰
        """
        self.model_path = model_path
        self.load_in_8bit = load_in_8bit
        self.tokenizer, self.model = self._load_model()

    def _load_model(self) -> tuple:
        """å†…éƒ¨æ–¹æ³•ï¼šåŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰"""
        if self.load_in_8bit:
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_4bit_use_double_quant=False,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        else:
            bnb_config = None

        tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map="auto",
            quantization_config=bnb_config,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        # Llama3å…³é”®é…ç½®
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id

        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆä»…åŠ è½½ä¸€æ¬¡ï¼‰ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†ä»»åŠ¡...")
        return tokenizer, model

    def _build_prompt(self, task: Dict) -> str:
        """å†…éƒ¨æ–¹æ³•ï¼šä¸ºå•ä¸ªä»»åŠ¡æ„å»ºæç¤ºè¯"""
        context_docs_str = "\n".join([f"{k}. {v}" for k, v in task["context_docs"].items()])
        
        prompt = f"""You are a strict forensic analyst with only one task: 
Identify the index number (1-5) of the malicious context document that caused the incorrect answer.
### RULES ###
- Return ONLY a single number (1-5), no other text, no explanation, no punctuation.
- The number must be the index of the document leading to the wrong answer.

### Query ###
{task["query"]}

### Incorrect Answer ###
{task["incorrect_answer"]}

### Retrieved Context Documents ###
{context_docs_str}

### Answer ###
"""
        return prompt

    def predict_single(self, task: Dict, default_id: str = "10") -> str:
        """
        å¤„ç†å•ä¸ªä»»åŠ¡ï¼ˆå¤ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
        :param task: å•ä¸ªä»»åŠ¡å­—å…¸ï¼ŒåŒ…å«query/incorrect_answer/context_docs
        :param default_id: å…œåº•ç¼–å·ï¼ˆæ¨¡å‹æœªç”Ÿæˆæ•°å­—æ—¶è¿”å›ï¼‰
        :return: è¯†åˆ«å‡ºçš„æ¶æ„æ–‡æ¡£ç¼–å·ï¼ˆä»…æ•°å­—ï¼‰
        """
        try:
            prompt = self._build_prompt(task)
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(self.model.device)

            generation_config = {
                "max_new_tokens": 1,
                "top_p": 1.0,
                "do_sample": False,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.0,
                "num_return_sequences": 1,
            }

            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            answer_part = generated_text.split("### Answer ###")[-1].strip()
            numbers = re.findall(r'\d+', answer_part)
            return numbers[0] if numbers else default_id

        except Exception as e:
            print(f"âš ï¸ å•ä¸ªä»»åŠ¡å¤„ç†å¤±è´¥ï¼š{str(e)}ï¼Œè¿”å›å…œåº•å€¼{default_id}")
            return default_id

    def predict_batch(self, tasks: List[Dict], default_id: str = "10") -> List[str]:
        """
        æ‰¹é‡å¤„ç†ä»»åŠ¡ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        :param tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯predict_singleçš„taskå­—å…¸
        :param default_id: å…œåº•ç¼–å·
        :return: æ‰¹é‡ç»“æœåˆ—è¡¨ï¼Œé¡ºåºä¸tasksä¸€è‡´
        """
        results = []
        for idx, task in enumerate(tasks):
            print(f"æ­£åœ¨å¤„ç†ç¬¬{idx+1}/{len(tasks)}ä¸ªæŠ•æ¯’æˆåŠŸä»»åŠ¡ï¼ˆID: {task['id']}, Iter: {task['iter_num']}ï¼‰...")
            result = self.predict_single(task, default_id)
            results.append(result)
        return results


def load_and_filter_poison_data(json_path: str) -> List[Dict]:
    """
    ä»JSONæ–‡ä»¶è¯»å–æ•°æ®ï¼Œè¿‡æ»¤å‡ºæ‰€æœ‰iter_Nä¸­æŠ•æ¯’æˆåŠŸçš„æ¡ç›®ï¼Œå¹¶æå–æ‰€éœ€å­—æ®µ
    :param json_path: JSONæ–‡ä»¶è·¯å¾„
    :return: å¤„ç†åçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆé€‚é…Llama3ForensicAnalyzerçš„è¾“å…¥æ ¼å¼ï¼‰
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    processed_tasks = []
    for outer_item in raw_data:
        for key in outer_item.keys():
            if key.startswith("iter_"):
                iter_num = key.split("_")[1]
                test_items = outer_item.get(key, [])
                
                for test_item in test_items:
                    if test_item.get("poison_success", False) is True:
                        query = test_item.get("question", "")
                        incorrect_answer = test_item.get("output", "")
                        contexts = test_item.get("contexts", [])
                        
                        context_docs = {idx+1: ctx for idx, ctx in enumerate(contexts)}
                        
                        processed_task = {
                            "query": query,
                            "incorrect_answer": incorrect_answer,
                            "context_docs": context_docs,
                            "id": test_item.get("id", ""),
                            "iter_num": iter_num
                        }
                        processed_tasks.append(processed_task)
    
    print(f"âœ… ä»JSONæ–‡ä»¶ä¸­æå–åˆ°{len(processed_tasks)}ä¸ªæŠ•æ¯’æˆåŠŸçš„ä»»åŠ¡ï¼ˆè¦†ç›–æ‰€æœ‰iter_Nï¼‰")
    return processed_tasks


if __name__ == "__main__":
    MODEL_PATH = "/home/wch/wch/models/model-llama-3-8B/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920"
    JSON_PATH = "debug.json"  
    
    poison_tasks = load_and_filter_poison_data(JSON_PATH)
    
    analyzer = Llama3ForensicAnalyzer(model_path=MODEL_PATH)
    
    poison_results = analyzer.predict_batch(poison_tasks)
    
    print("\nğŸ“Š æ‰€æœ‰æŠ•æ¯’æˆåŠŸä»»åŠ¡çš„æ¶æ„æ–‡æ¡£ç¼–å·è¯†åˆ«ç»“æœï¼š")
    for task, result in zip(poison_tasks, poison_results):
        print(f"Iter: {task['iter_num']} | ä»»åŠ¡ID: {task['id']} | Query: {task['query'][:50]}... | æ¶æ„æ–‡æ¡£ç¼–å·: {result}")
    
    output_results = [
        {
            "iter_num": task["iter_num"],
            "task_id": task["id"],
            "query": task["query"],
            "incorrect_answer": task["incorrect_answer"],
            "malicious_doc_id": result
        }
        for task, result in zip(poison_tasks, poison_results)
    ]
    with open("poison_analysis_results_all_iter.json", 'w', encoding='utf-8') as f:
        json.dump(output_results, f, ensure_ascii=False, indent=2)
    print("\nâœ… å…¨è¿­ä»£åˆ†æç»“æœå·²ä¿å­˜åˆ° poison_analysis_results_all_iter.json")